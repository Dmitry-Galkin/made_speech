{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RcyxmRJGqlY"
   },
   "source": [
    "# Практика №4\n",
    "\n",
    "Теперь мы построим и обучим простую end-to-end модель. Будем работать с пропатченной версией уже готового [пайплайна](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch). Также нам пригодится [ESPnet](https://github.com/espnet/espnet) для использования модели [Transformer](http://jalammar.github.io/illustrated-transformer/) в качестве энкодера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDbO_rrWGq7j"
   },
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzJyomV1JaLp",
    "outputId": "9be62930-a27d-431c-d1de-c444f6df1cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchaudio) (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TROAsHTXHWik",
    "outputId": "812c79f3-eab6-4d8f-9ccd-7f996d1365cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6\n",
      "To: /content/lab4.zip\n",
      "100% 2.77M/2.77M [00:00<00:00, 178MB/s]\n",
      "replace lab4/train_clean_100_text_clean.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
      "/content/lab4\n"
     ]
    }
   ],
   "source": [
    "!gdown --id '1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6'\n",
    "\n",
    "!unzip -q lab4.zip\n",
    "!rm -rf lab4.zip sample_data\n",
    "%cd lab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m4wcCtkIH2dn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from utils import TextTransform\n",
    "from utils import cer\n",
    "from utils import wer\n",
    "\n",
    "from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
    "# from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
    "from espnet.nets.pytorch_backend.conformer.encoder_layer import EncoderLayer\n",
    "from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
    "from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
    "from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n",
    "from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
    "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHKuY8HnAC4M",
    "outputId": "78883516-bb9d-4748-fdac-bcfb1d94f518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  7 09:50:50 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NaESUZiHJgfN"
   },
   "outputs": [],
   "source": [
    "train_audio_transforms = torch.nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000,\n",
    "                                                              n_fft=400,\n",
    "                                                              hop_length=160,\n",
    "                                                              n_mels=80)\n",
    "\n",
    "text_transform = TextTransformBPE(\n",
    "    path_to_text=\"/content/lab4/train_clean_100_text_clean.txt\"\n",
    ")\n",
    "\n",
    "#-----------------------------TODO №2-----------------------------------\n",
    "# Заменить графемный токенайзер на сабвордовый TextTransformBPE\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0])\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=2000, collapse_repeated=True):\n",
    "\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9OqoVLnrJsCV"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=80,\n",
    "        output_size=2001,\n",
    "        conv2d_filters=32,\n",
    "        attention_dim=240,\n",
    "        attention_heads=8,\n",
    "        feedforward_dim=512,\n",
    "        num_layers=8,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.conv_in = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "            # torch.nn.BatchNorm2d(conv2d_filters),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "            # torch.nn.BatchNorm2d(conv2d_filters),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "            # torch.nn.BatchNorm2d(conv2d_filters),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.conv_out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(conv2d_filters * (input_size // 8), attention_dim),\n",
    "            PositionalEncoding(attention_dim, 0.1),\n",
    "        )\n",
    "        positionwise_layer = PositionwiseFeedForward\n",
    "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
    "        self.encoder_layer = repeat(\n",
    "            num_layers,\n",
    "            lambda lnum: EncoderLayer(\n",
    "                attention_dim,\n",
    "                MultiHeadedAttention(\n",
    "                    attention_heads, attention_dim, dropout\n",
    "                ),\n",
    "                positionwise_layer(*positionwise_layer_args),\n",
    "                positionwise_layer(*positionwise_layer_args),  # feed_forward_macaron\n",
    "                None,\n",
    "                dropout,\n",
    "                normalize_before=True,\n",
    "                concat_after=False,\n",
    "            ),\n",
    "        )\n",
    "        self.after_norm = LayerNorm(attention_dim)\n",
    "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
    "\n",
    "    def forward(self, x, ilens):\n",
    "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
    "        x = self.conv_in(x)\n",
    "        b, c, t, f = x.size()\n",
    "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
    "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::8].to(x.device)\n",
    "        x, _, _ = self.encoder_layer(x, masks)\n",
    "        x = self.after_norm(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhbOKdLLbXHa",
    "outputId": "839a31e8-3400-4106-9c78-26e73f84bfe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 2001])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 800, 80)\n",
    "model = TransformerModel()\n",
    "output = model(x, [800, 90])\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d2p_8IjeKkqq"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms[:, :, :,:max(input_lengths)].to(device), labels.to(device) #(batch, 1, feat_dim, time)\n",
    "        spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch, time, feat_dim,)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "        input_lengths = [x // 8 for x in input_lengths]\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if batch_idx % 500 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(spectrograms),\n",
    "                data_len,\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item(),\n",
    "                scheduler.get_last_lr()[0]))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, decode=True):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch time, feat_dim,)\n",
    "            \n",
    "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "            input_lengths = [x // 8 for x in input_lengths]\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            if decode:\n",
    "              decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels.int(), label_lengths)\n",
    "              for j in range(len(decoded_preds)):\n",
    "                  test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                  test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "    if decode:\n",
    "        avg_cer = sum(test_cer)/len(test_cer)\n",
    "        avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}, Average CER: {avg_cer:4f} Average WER: {avg_wer:.4f}\\n\")\n",
    "    else:\n",
    "        print(f\"Average loss: {test_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MzEbtsB1LKsh"
   },
   "outputs": [],
   "source": [
    "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
    "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
    "    \n",
    "    hparams = {\n",
    "        \"input_size\": 80,\n",
    "        \"output_size\": 2001,\n",
    "        \"conv2d_filters\": 32,\n",
    "        \"attention_dim\": 512,  # 320,\n",
    "        \"attention_heads\": 8,\n",
    "        \"feedforward_dim\": 2048,  # 1024,\n",
    "        \"num_layers\": 12,  # 10\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=test_batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        hparams['input_size'],\n",
    "        hparams['output_size'],\n",
    "        hparams['conv2d_filters'],\n",
    "        hparams['attention_dim'],\n",
    "        hparams['attention_heads'],\n",
    "        hparams['feedforward_dim'],\n",
    "        hparams['num_layers'],\n",
    "        hparams['dropout']).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = torch.nn.CTCLoss(blank=2000, zero_infinity=False).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        !date\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "        test(model, device, test_loader, criterion, epoch, decode=not(epoch % 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "21e143267b6d4460ae0960096ada46f8",
      "a012cf4456b2410dae05f6cd9873c6b4",
      "703d28ec02f64f68a337082aa2015bff",
      "48b6bbce65cf4f80bf2615784f70f2a1",
      "e15e095083f84238930383f3a4412a6d",
      "ec4aae265e344c55a1a6cd3d05f7de82",
      "4d5546d64c5548538ec2ebd4d0d5bfd9",
      "be3748c9cf454e51b13c5631c1d671e2",
      "818c4f41b66949faac259ca54b66010d",
      "8b4530949c8242a9980e416a54191c5d",
      "5f24ee994d114b9bb543192f2b685377",
      "65f44f09e1874a198aad5506d279e697",
      "c700466e04ad409ebb4973292ad120c1",
      "d00d9426aebe4172a1c83814e14945a7",
      "2632d65778aa45f0815fcb39855912ca",
      "108c121df0024c5fb8b026347fc1cbdf",
      "ed8b0a2033864c1fbf62bb9fdb861d8a",
      "17479df50a39436aa18e93d7d6b3acb8",
      "120d0c85250548eb8d278e18765aa430",
      "07ef6d142e1c44138e5cf52763cd271f",
      "f4e65d49a7a94d30b1bc0ac4c7138124",
      "e2315018c3ad405baaca1b25486ee5e3"
     ]
    },
    "id": "eExZLsUiLeXk",
    "outputId": "b15642de-5722-43b1-a854-78982867bf8f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e143267b6d4460ae0960096ada46f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/5.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f44f09e1874a198aad5506d279e697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (conv_in): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (conv_out): Sequential(\n",
      "    (0): Linear(in_features=320, out_features=512, bias=True)\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer): MultiSequential(\n",
      "    (0): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (3): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (4): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (5): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (6): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (7): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (8): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (9): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (10): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (11): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (feed_forward_macaron): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm_ff): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_mha): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm_ff_macaron): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "  (final_layer): Linear(in_features=512, out_features=2001, bias=True)\n",
      ")\n",
      "Num Model Parameters 64248145\n",
      "Tue Jun  7 09:56:44 UTC 2022\n",
      "Train Epoch: 1 [0/28539 (0%)]\tLoss: 23.638412\tLR: 0.000040\n",
      "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 6.617467\tLR: 0.000130\n",
      "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 6.586646\tLR: 0.000220\n",
      "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 6.483624\tLR: 0.000309\n",
      "\n",
      "evaluating...\n",
      "Average loss: 6.0640\n",
      "\n",
      "Tue Jun  7 10:06:16 UTC 2022\n",
      "Train Epoch: 2 [0/28539 (0%)]\tLoss: 6.308908\tLR: 0.000360\n",
      "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 5.484544\tLR: 0.000450\n",
      "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 5.037552\tLR: 0.000540\n",
      "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 4.549785\tLR: 0.000629\n",
      "\n",
      "evaluating...\n",
      "Average loss: 4.0860\n",
      "\n",
      "Tue Jun  7 10:15:04 UTC 2022\n",
      "Train Epoch: 3 [0/28539 (0%)]\tLoss: 4.300642\tLR: 0.000680\n",
      "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 4.318555\tLR: 0.000770\n",
      "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 4.105318\tLR: 0.000860\n",
      "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 4.051180\tLR: 0.000949\n",
      "\n",
      "evaluating...\n",
      "Average loss: 3.6278\n",
      "\n",
      "Tue Jun  7 10:23:52 UTC 2022\n",
      "Train Epoch: 4 [0/28539 (0%)]\tLoss: 4.290630\tLR: 0.001000\n",
      "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 3.980925\tLR: 0.000960\n",
      "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 3.832464\tLR: 0.000920\n",
      "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 3.635855\tLR: 0.000880\n",
      "\n",
      "evaluating...\n",
      "Average loss: 3.2583\n",
      "\n",
      "Tue Jun  7 10:32:39 UTC 2022\n",
      "Train Epoch: 5 [0/28539 (0%)]\tLoss: 3.868752\tLR: 0.000857\n",
      "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 3.473433\tLR: 0.000817\n",
      "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 3.378432\tLR: 0.000777\n",
      "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 3.303911\tLR: 0.000737\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.0370, Average CER: 0.468388 Average WER: 0.6897\n",
      "\n",
      "Tue Jun  7 10:45:28 UTC 2022\n",
      "Train Epoch: 6 [0/28539 (0%)]\tLoss: 3.451173\tLR: 0.000714\n",
      "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 3.473808\tLR: 0.000674\n",
      "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 3.327214\tLR: 0.000634\n",
      "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 3.148521\tLR: 0.000594\n",
      "\n",
      "evaluating...\n",
      "Average loss: 2.6415\n",
      "\n",
      "Tue Jun  7 10:54:09 UTC 2022\n",
      "Train Epoch: 7 [0/28539 (0%)]\tLoss: 3.110171\tLR: 0.000571\n",
      "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 3.014616\tLR: 0.000531\n",
      "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 3.152010\tLR: 0.000491\n",
      "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 2.865904\tLR: 0.000451\n",
      "\n",
      "evaluating...\n",
      "Average loss: 2.4332\n",
      "\n",
      "Tue Jun  7 11:02:52 UTC 2022\n",
      "Train Epoch: 8 [0/28539 (0%)]\tLoss: 2.722960\tLR: 0.000428\n",
      "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 2.585833\tLR: 0.000388\n",
      "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 2.442927\tLR: 0.000348\n",
      "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 2.440639\tLR: 0.000308\n",
      "\n",
      "evaluating...\n",
      "Average loss: 2.2320\n",
      "\n",
      "Tue Jun  7 11:11:35 UTC 2022\n",
      "Train Epoch: 9 [0/28539 (0%)]\tLoss: 2.848556\tLR: 0.000286\n",
      "Train Epoch: 9 [8000/28539 (28%)]\tLoss: 2.343999\tLR: 0.000246\n",
      "Train Epoch: 9 [16000/28539 (56%)]\tLoss: 2.414434\tLR: 0.000205\n",
      "Train Epoch: 9 [24000/28539 (84%)]\tLoss: 2.422889\tLR: 0.000165\n",
      "\n",
      "evaluating...\n",
      "Average loss: 2.0819\n",
      "\n",
      "Tue Jun  7 11:20:15 UTC 2022\n",
      "Train Epoch: 10 [0/28539 (0%)]\tLoss: 2.286582\tLR: 0.000143\n",
      "Train Epoch: 10 [8000/28539 (28%)]\tLoss: 2.199406\tLR: 0.000103\n",
      "Train Epoch: 10 [16000/28539 (56%)]\tLoss: 2.271030\tLR: 0.000063\n",
      "Train Epoch: 10 [24000/28539 (84%)]\tLoss: 2.050702\tLR: 0.000023\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.0098, Average CER: 0.299404 Average WER: 0.5351\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "test_batch_size = 8\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mby39YVqZadd"
   },
   "source": [
    "### <b>Задание №1</b> (5 баллов):\n",
    "На данный момент практически все E2E SOTA решения используют [сабворды](https://dyakonov.org/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/) (subwords/wordpieces) в качестве таргетов нейронки для распознавания. Нам бы тоже не мешало перейти от графем к сабвордам. Теперь вместо букв (графем) будем распознавать кусочки слов. В качестве такого токенайзера предлагается использовать [Sentencepiece](https://github.com/google/sentencepiece). Пример обучения BPE токенайзера можно найти в [link](https://github.com/google/sentencepiece/tree/master/python). Главное правильно обернуть его в наш класс TextTransformBPE. Текстовый файл (train_clean_100_text_clean.txt) для обучения токенайзера уже подготовлен и лежит в корневой папке проекта. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFMTUcyCMc5Q",
    "outputId": "2954741e-2132-4d36-f571-aca3b5d5cb46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JNbiW919e2le"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class TextTransformBPE:\n",
    "    def __init__(self, path_to_text):\n",
    "        \"\"\" Обучение BPE модели на 2000 юнитов.\"\"\"\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input=path_to_text, \n",
    "            model_prefix='m', \n",
    "            vocab_size=2000,\n",
    "            model_type=\"bpe\"\n",
    "        )\n",
    "        self.sp = spm.SentencePieceProcessor(\n",
    "            model_file='m.model'\n",
    "        )\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Преобразование входного текста в последовательность сабвордов в формате их индекса в BPE модели \"\"\"\n",
    "        # int_sequence = self.sp.encode_as_ids(text)\n",
    "        int_sequence = self.sp.encode(text)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Преобразование последовательности индексов сабвордов в текст \"\"\"\n",
    "        # string = self.sp.decode_ids(labels)\n",
    "        string = self.sp.decode(labels)\n",
    "        return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFS3PNxEAUiK"
   },
   "source": [
    "### **Ответ**:\n",
    "Код для обучения BPE токенайзера находится в ячейке выше. \n",
    "\n",
    "Создание объекта (экзмепляра класса TextTransformBPE) происходит в ячейке **7**. Также в коде были изменены значения параметров **blank** с 28 на 2000 и **output_size** с 29 на 2001.\n",
    "\n",
    "Результаты (значение WER, CTC и CER) при использовании графем и сабвордов приведены в таблице ниже. Все метрики взяты на конец 10 эпохи на тестовом множестве. Кроме таргетов модели ничем не отличаются.\n",
    "\n",
    "||WER|CER|CTC|\n",
    "|-|-|-|-|\n",
    "|Графемы|0.54|0.18|0.67|\n",
    "|Сабворды|0.51|0.31|1.95|\n",
    "\n",
    "WER улучшилось всего на 3%, хотя не покидает чувство, что что-то сделал не так. Т.к. коллеги в чате писали, что у них на 6% улучшилось качество. При этом метрики CER и CTC у меня стали хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNaEAv6L5EeQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV48Q7HqZsAD"
   },
   "source": [
    "### <b>Задание №2</b> (5 баллов):\n",
    "Импровизация по улучшению качества распознавания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1OBVhLgATV_"
   },
   "source": [
    "### **Ответ**:\n",
    "\n",
    "Здесь приведены изменения в архитектуре модели и результаты, которые удалось получить с этими изменениями. В качестве результатов будет использоваться только значение **WER** на конец 10-й эпохи (один раз запустил на ночь на 20 эпох, но colab благоразумно решил прервать сессию на 11 эпохе)\n",
    "\n",
    "> **1)** Первоначально добавил в сверточную часть еще одну свертку со страйдом 2, аналогичную предыдущей свертке (*ячейка 8*). Таким образом, размер выход сверточной части уменьшился в 8 раз. И здесь же еще изменил размеры батчей: **bacth_size=24**, **test_batch_size=8**. Полученный результат:\n",
    "$${WER=0.46}$$\n",
    "Качество улушилось на **5%**, что хорошо. Поэтому данное изменение в нашей модели сохраним.\n",
    "\n",
    "\n",
    "> **2)** В дополнении к первому пунтку были изменены параметры трансформера: **attention_dim=512**, **feedforward_dim=2048**, **num_layers=12** и размер батча на обучении взял побольше: **bacth_size=32**. Полученный результат:\n",
    "$${WER=0.4}$$\n",
    "Качество улучшилось еще на **6%**.\n",
    "\n",
    "\n",
    "> **3)** Добавил Conformer вместо Transformer в качестве EncoderLayer (*ячейка 8*). Параметры **feed_forward_macaron** и **conv_module** выставлял равными **None** (как в исходниках значения по умолчанию: https://espnet.github.io/espnet/_modules/espnet/nets/pytorch_backend/conformer/encoder.html). Полученный результат:\n",
    "$${WER=0.39}$$\n",
    "Еще на **1%** удалось улучшить.\n",
    "\n",
    "> **4)** А здесь приведены изменения, которые только ухудшили результат, по сравнению с исходным, полученным в прошлом пункте.\n",
    "> * Увеличение максимального *learning_rate* до *1e-2*\n",
    "> * Добавление батч нормализации в сверточных слоях + увеличение *num_layers* до 12.\n",
    "> * Добавление батч нормализации и увеличение максимального *learning_rate* до *3e-5* (была идея в том, что батч нормализация вроде как должна ускорять сходимость, поэтому можно learning_rate сделать побольше)\n",
    "> * Conformer в качестве EncoderLayer и **feed_forward_macaron = positionwise_layer(*positionwise_layer_args)**, как все в тех же исходниках. Но **conv_module = None**.\n",
    "\n",
    "> **5)** Вероятно, увеличение количества эпох приведет к улучшению качества. Но при прочих равных на 10 эпохах максимум удалось достичь значения **WER=39%**, что на **15%** лучше, чем самый первый результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrKmSCUY7hIi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "asr_lab_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07ef6d142e1c44138e5cf52763cd271f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "108c121df0024c5fb8b026347fc1cbdf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "120d0c85250548eb8d278e18765aa430": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17479df50a39436aa18e93d7d6b3acb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "21e143267b6d4460ae0960096ada46f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a012cf4456b2410dae05f6cd9873c6b4",
       "IPY_MODEL_703d28ec02f64f68a337082aa2015bff",
       "IPY_MODEL_48b6bbce65cf4f80bf2615784f70f2a1"
      ],
      "layout": "IPY_MODEL_e15e095083f84238930383f3a4412a6d"
     }
    },
    "2632d65778aa45f0815fcb39855912ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4e65d49a7a94d30b1bc0ac4c7138124",
      "placeholder": "​",
      "style": "IPY_MODEL_e2315018c3ad405baaca1b25486ee5e3",
      "value": " 331M/331M [00:14&lt;00:00, 24.1MB/s]"
     }
    },
    "48b6bbce65cf4f80bf2615784f70f2a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b4530949c8242a9980e416a54191c5d",
      "placeholder": "​",
      "style": "IPY_MODEL_5f24ee994d114b9bb543192f2b685377",
      "value": " 5.95G/5.95G [03:51&lt;00:00, 30.7MB/s]"
     }
    },
    "4d5546d64c5548538ec2ebd4d0d5bfd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f24ee994d114b9bb543192f2b685377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65f44f09e1874a198aad5506d279e697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c700466e04ad409ebb4973292ad120c1",
       "IPY_MODEL_d00d9426aebe4172a1c83814e14945a7",
       "IPY_MODEL_2632d65778aa45f0815fcb39855912ca"
      ],
      "layout": "IPY_MODEL_108c121df0024c5fb8b026347fc1cbdf"
     }
    },
    "703d28ec02f64f68a337082aa2015bff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be3748c9cf454e51b13c5631c1d671e2",
      "max": 6387309499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_818c4f41b66949faac259ca54b66010d",
      "value": 6387309499
     }
    },
    "818c4f41b66949faac259ca54b66010d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b4530949c8242a9980e416a54191c5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a012cf4456b2410dae05f6cd9873c6b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec4aae265e344c55a1a6cd3d05f7de82",
      "placeholder": "​",
      "style": "IPY_MODEL_4d5546d64c5548538ec2ebd4d0d5bfd9",
      "value": "100%"
     }
    },
    "be3748c9cf454e51b13c5631c1d671e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c700466e04ad409ebb4973292ad120c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed8b0a2033864c1fbf62bb9fdb861d8a",
      "placeholder": "​",
      "style": "IPY_MODEL_17479df50a39436aa18e93d7d6b3acb8",
      "value": "100%"
     }
    },
    "d00d9426aebe4172a1c83814e14945a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_120d0c85250548eb8d278e18765aa430",
      "max": 346663984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07ef6d142e1c44138e5cf52763cd271f",
      "value": 346663984
     }
    },
    "e15e095083f84238930383f3a4412a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2315018c3ad405baaca1b25486ee5e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec4aae265e344c55a1a6cd3d05f7de82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed8b0a2033864c1fbf62bb9fdb861d8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4e65d49a7a94d30b1bc0ac4c7138124": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
